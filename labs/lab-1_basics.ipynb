{"cells":[{"cell_type":"markdown","metadata":{"id":"bvScrL3XSDUb"},"source":["# Lab1 - Deep learning architectures for Image Classification\n","\n","In this tutorial, we will see some classic architectures of convolutional networks and how to use pre-trained networks to speed-up training on new tasks.\n","\n","The first two sections are an introduction to Git/GitHub and Pytorch. The last part focuses on Deep learning architecture."]},{"cell_type":"markdown","metadata":{"id":"ckLjoTgpb7fD"},"source":["## First part: Introduction to Git and GitHub\n","\n","This notebook will guide you through the essential commands and operations needed to use Git and GitHub effectively. By the end of this notebook, you will be able to:\n","\n","1. Install and configure Git.\n","2. Create a local repository.\n","3. Make commits to your repository.\n","4. Push your repository to GitHub.\n","5. Collaborate with others using GitHub.\n","\n","### Step 1: Install and Configure Git\n","\n","First, you need to install Git on your computer. You can download it from [git-scm.com](https://git-scm.com/).\n","\n","Once installed, open your terminal (Command Prompt, PowerShell, or Git Bash) and configure Git with your username and email:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"shellscript"},"id":"1nYILKoRb7fE"},"outputs":[],"source":["git config --global user.name \"Your Name\"\n","git config --global user.email \"your.email@email.com\""]},{"cell_type":"markdown","metadata":{"id":"RG8vWV4Qb7fF"},"source":["### Step 2: Create a Local Repository\n","\n","1. Create a new directory for your project and navigate into it:"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"shellscript"},"id":"EJTyYH4Gb7fG"},"outputs":[],"source":["mkdir my_project\n","cd my_project"]},{"cell_type":"markdown","metadata":{"id":"O69QVEchb7fG"},"source":["2. Initialize a new Git repository:"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"shellscript"},"id":"GQpF6eCrb7fH"},"outputs":[],"source":["git init"]},{"cell_type":"markdown","metadata":{"id":"bwc874TJb7fH"},"source":["### Step 3: Make Commits to Your Repository\n","\n","1. Create a new file (e.g., README.md) and add some content to it"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"shellscript"},"id":"liUBnaC7b7fI"},"outputs":[],"source":["echo \"# My Project\" > README.md"]},{"cell_type":"markdown","metadata":{"id":"i_jh5vvMb7fJ"},"source":["2. Check the status of your repository:"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"shellscript"},"id":"W-xJnKH4b7fK"},"outputs":[],"source":["git status"]},{"cell_type":"markdown","metadata":{"id":"rQBVW9jIb7fL"},"source":["\n","3. Add the file to the staging area:"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"shellscript"},"id":"fgvAdGz4b7fL"},"outputs":[],"source":["git add README.md"]},{"cell_type":"markdown","metadata":{"id":"L_YLSnXmb7fL"},"source":["4. Commit the changes:"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"shellscript"},"id":"eawWO-plb7fL"},"outputs":[],"source":["git commit -m \"Initial commit\""]},{"cell_type":"markdown","metadata":{"id":"M9jJKNqnb7fL"},"source":["### Step 4: Push Your Repository to GitHub\n","\n","1. Create a new repository on GitHub. Do not initialize the repository with a README, .gitignore, or license.\n","\n","2. Add the remote repository URL to your local repository:"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"shellscript"},"id":"zBIXQBQVb7fL"},"outputs":[],"source":["git remote add origin https://github.com/yourusername/my_project.git"]},{"cell_type":"markdown","metadata":{"id":"BdaZ1ZtGb7fN"},"source":["3. Push your changes to the remote repository:"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"shellscript"},"id":"wx8tejDWb7fN"},"outputs":[],"source":["git push -u origin master"]},{"cell_type":"markdown","metadata":{"id":"ZKRLzSUwb7fN"},"source":["### Step 5: Collaborate with Others Using GitHub\n","\n","To collaborate with others, you can add collaborators to your GitHub repository. Go to the repository settings and add collaborators.\n","\n","Collaborators can clone the repository to their local machine:"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"shellscript"},"id":"HZ1p3OnUb7fN"},"outputs":[],"source":["git clone https://github.com/yourusername/my_project.git"]},{"cell_type":"markdown","metadata":{"id":"JGFDnxrfb7fN"},"source":["Collaborators can create a new branch to work on a feature (A simple rule is: \"one new feature, one branch, one pull request\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"shellscript"},"id":"aRGbrBMGb7fN"},"outputs":[],"source":["git checkout -b feature-branch"]},{"cell_type":"markdown","metadata":{"id":"aMdYsnM3b7fN"},"source":["They can make changes and commit them:"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"shellscript"},"id":"lxHY8oqGb7fN"},"outputs":[],"source":["git add .\n","git commit -m \"Add new feature\""]},{"cell_type":"markdown","metadata":{"id":"pbaypiqPb7fN"},"source":["Push the branch to the remote repository:"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"shellscript"},"id":"QheKGp2mb7fN"},"outputs":[],"source":["git push origin feature-branch"]},{"cell_type":"markdown","metadata":{"id":"HvCza3T7b7fN"},"source":["Create a pull request on GitHub to merge the branch into the main branch.\n","\n","1. Click on \"Contribute\" and \"Open pull request\".\n","\n","2. Select the Base (the branch you want to merge your changes into, typically the `main` or `master` branch.) and Compare (the branch that contains your changes) ranches:\n","\n","3. Review the Changes: GitHub will show you the changes between the two branches. Review these changes to ensure everything looks correct.\n","\n","4. Fill out the pull request form and create the pull request:\n","\n","5. Review and Merge:\n","\n","The pull request will now be visible to collaborators, who can review your changes, leave comments, and request modifications. If reviewers request changes, make the necessary modifications in your branch, commit them, and push the updates to the same branch. The pull request will automatically update with the new changes. Once the pull request is approved, you or a collaborator with merge permissions can merge the pull request into the base branch.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"A_5Vibo6b7fN"},"source":["### Additional Git Commands\n","\n","- Check the status of your repository:"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"shellscript"},"id":"CFMGWgD4b7fO"},"outputs":[],"source":["git status"]},{"cell_type":"markdown","metadata":{"id":"bTinN5wFb7fO"},"source":["- View the commit history:"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"shellscript"},"id":"oFDBVvutb7fO"},"outputs":[],"source":["git log"]},{"cell_type":"markdown","metadata":{"id":"HBLuaeMjb7fO"},"source":["- Switch to a different branch:"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"shellscript"},"id":"48fB1UKIb7fP"},"outputs":[],"source":["git checkout branch-name"]},{"cell_type":"markdown","metadata":{"id":"DDhmzZNzb7fP"},"source":["Merge a branch into the current branch:"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"shellscript"},"id":"_9HZEWWDb7fP"},"outputs":[],"source":["git merge branch-name"]},{"cell_type":"markdown","metadata":{"id":"qi7f0BAtb7fP"},"source":["- Fetch updates from the remote repository:"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"shellscript"},"id":"Ejno5RKlb7fQ"},"outputs":[],"source":["git fetch"]},{"cell_type":"markdown","metadata":{"id":"JN8FUjXZb7fQ"},"source":["- Pull updates from the remote repository and merge them into the current branch:"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"shellscript"},"id":"m0zJGsg1b7fQ"},"outputs":[],"source":["git pull"]},{"cell_type":"markdown","metadata":{"id":"IGRJZik6b7fQ"},"source":["## Second part: Introduction to PyTorch\n","\n","PyTorch is an open-source machine learning library based on the Torch library, used for applications such as computer vision and natural language processing, primarily as a research platform that provides dynamic computation graphs and rich ecosystems of tools and libraries.\n","\n","In this section, we will:\n","1. Introduce PyTorch and its basic concepts.\n","2. Learn how to manipulate tensors.\n","3. Define a simple Multi-Layer Perceptron (MLP).\n","4. Train the MLP on a simple dataset.\n","\n","First, let's download and import PyTorch and check its version."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eqjLK3zRb7fQ"},"outputs":[],"source":["!pip install torch torchvision torchaudio numpy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mJ5DVEZUb7fQ"},"outputs":[],"source":["import torch\n","print(f\"PyTorch version: {torch.__version__}\")"]},{"cell_type":"markdown","metadata":{"id":"Q6sgO1m9b7fR"},"source":["### 1. Tensors\n","\n","Tensors are the core data structure in PyTorch. They are similar to NumPy arrays but with added functionality for GPU acceleration and automatic differentiation. Tensors are optimized for automatic differentiation (we will see more about that later in the Gradient section).\n","\n","A. Creating Tensors\n","\n","Tensor can be created\n","- directly from data. The data type is automatically inferred.\n","- from NumPy arrays\n","- from another tensor\n","- with random or constant values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h-bdTRV1b7fR"},"outputs":[],"source":["# Creating a tensor from a list\n","x_from_list = torch.tensor([1, 2, 3, 4, 5])\n","print(f\"Tensor from list: \\n {x_from_list}\")\n","\n","# Creating a tensor from a numpy array\n","import numpy as np\n","numpy_array = np.array([1, 2, 3, 4, 5])\n","x_from_numpy = torch.tensor(numpy_array)\n","\n","\n","# Creating a tensor from another tensor\n","x_ones = torch.ones_like(x_from_list)\n","print(f\"Ones Tensor: \\n {x_from_list} \\n\")\n","\n","x_rand = torch.rand_like(x_from_list, dtype=torch.float)\n","print(f\"Random Tensor: \\n {x_rand} \\n\")\n","\n","\n","# Creating a tensor with random values\n","x_random = torch.rand((3, 3))\n","print(f\"Tensor with random values: \\n {x_random} \\n\")\n","\n","\n","# Creating a tensor with constant values\n","x_zeros = torch.zeros((3, 3))\n","print(f\"Tensor with zeros:\\n {x_zeros} \\n\")\n","\n","x_ones = torch.ones((3, 3))\n","print(f\"Tensor with ones: \\n {x_ones} \\n\")"]},{"cell_type":"markdown","metadata":{"id":"hpGBo1C_b7fR"},"source":["Tensor attributes describe their shape, datatype, and the device on which they are stored."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mwgqWbhlb7fR"},"outputs":[],"source":["tensor = torch.rand(3,4)\n","\n","print(f\"Shape of tensor: {tensor.shape}\")\n","print(f\"Datatype of tensor: {tensor.dtype}\")\n","print(f\"Device tensor is stored on: {tensor.device}\")"]},{"cell_type":"markdown","metadata":{"id":"BcPKwW2Rb7fS"},"source":["We notice that the variable `tensor` has 'cpu' as device attribute. PyTorch allows you to store tensors and perform computations on different devices, such as the CPU and GPU. Using a GPU can significantly speed up training and inference for large models and datasets.\n","\n","By default, tensors are created on the CPU. We need to explicitly move tensors to the GPU using .to method. First, let's check if a GPU is available."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X88t-VWDb7fS"},"outputs":[],"source":["# Check if GPU is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")"]},{"cell_type":"markdown","metadata":{"id":"DcALKR_6b7fS"},"source":["If you’re using Colab, you can allocate a GPU by going to Runtime > Change runtime type > GPU.\n","You can move tensors to a specific device using the .to(device) method."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ktGj4FFkb7fS"},"outputs":[],"source":["# Create a tensor (by default on the cpu)\n","tensor_cpu = torch.tensor([1.0, 2.0, 3.0])\n","print(f\"Tensor on device: {tensor_cpu.device}\")\n","\n","# Move the tensor to the selected device\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    tensor_device = tensor_cpu.to(device)\n","    print(f\"Tensor on device: {tensor_device.device}\")"]},{"cell_type":"markdown","metadata":{"id":"fuOPmsrDb7fT"},"source":["### 2. Tensor operations\n","\n","Lots of tensor operations, including arithmetic, linear algebra, matrix manipulation (transposing, indexing, slicing), sampling and more are already implemented in PyTorch. The next cells show some basic operations. You can find comprehensively list of available operations [here](https://pytorch.org/docs/stable/torch.html).\n","\n","Arithmetic operation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9V-iWvs_b7fT"},"outputs":[],"source":["# Adding two tensors\n","tensor_a = torch.tensor([1, 2, 3])\n","tensor_b = torch.tensor([4, 5, 6])\n","tensor_sum = tensor_a + tensor_b\n","print(f\"Sum of tensors: \\n {tensor_sum}\\n\")\n","\n","\n","# Multiplying two tensors element-wise. All three result tensors will have the same value\n","tensor_product_1 = tensor_a * tensor_b\n","tensor_product_2 = tensor_a.mul(tensor_b)\n","tensor_product_3 = torch.mul(tensor_a, tensor_b)\n","print(f\"Product of tensors: \\n {tensor_product_1} \\n\")\n","\n","y1 = tensor @ tensor.T\n","y2 = tensor.matmul(tensor.T)\n","\n","y3 = torch.rand_like(y1)\n","torch.matmul(tensor, tensor.T, out=y3)\n","\n","\n","# Matrix multiplication. All three result tensors will have the same value\n","tensor_c = torch.tensor([[1, 2], [3, 4]])\n","tensor_d = torch.tensor([[5, 6], [7, 8]])\n","tensor_matmul_1 = tensor_c.matmul(tensor_d)\n","tensor_matmul_2 = tensor_c @ tensor_d\n","tensor_matmul_3 = torch.matmul(tensor_c, tensor_d)\n","print(f\"Matrix multiplication: \\n {tensor_matmul_1} \\n\")"]},{"cell_type":"markdown","metadata":{"id":"hDspJolub7fT"},"source":["Indexing and Slicing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jFs5kGAxb7fT"},"outputs":[],"source":["# Creating a 2D tensor\n","tensor_2d = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n","print(f\"2D Tensor: \\n {tensor_2d} \\n\")\n","\n","# Indexing\n","print(f\"Element at (0, 0): \\n {tensor_2d[0, 0]} \\n\")\n","\n","# Slicing\n","print(f\"First row: \\n {tensor_2d[0, :]} \\n\")\n","print(f\"First column: \\n {tensor_2d[:, 0]} \\n\")"]},{"cell_type":"markdown","metadata":{"id":"vDXLFcCTb7fT"},"source":["Single tensor operations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qZkz1Vy8b7fU"},"outputs":[],"source":["tensor = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])\n","\n","## Statistical operations:\n","mean_value = tensor.mean()\n","print(f\"Mean of tensor: \\n {mean_value} - {type(mean_value)} \\n\")\n","mean_value_item = mean_value.item()\n","print(f\"Mean of tensor (item): \\n {mean_value_item} - {type(mean_value_item)}\\n\")\n","\n","# Sum of all elements\n","sum_value = tensor.sum()\n","print(f\"Sum of tensor: \\n {sum_value} \\n\")\n","\n","# Maximum value\n","max_value = tensor.max()\n","print(f\"Maximum value in tensor: \\n {max_value} \\n\")\n","\n","# Minimum value\n","min_value = tensor.min()\n","print(f\"Minimum value in tensor: \\n {min_value} \\n\")\n","\n","# Standard deviation\n","std_value = tensor.std()\n","print(f\"Standard deviation of tensor: \\n {std_value} \\n\")\n"]},{"cell_type":"markdown","metadata":{"id":"4tZbCQzBb7fU"},"source":["Concatenation and operation on the dimension"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3tbrouRXb7fU"},"outputs":[],"source":["tensor = torch.ones(4, 4)\n","print(f\"Initial tensor: \\n {tensor} \\n\")\n","\n","# Concatenation of tensors\n","t1 = torch.cat([tensor, tensor, tensor], dim=1)\n","print(f\"Concatenated tensor (dim=1): \\n {t1} \\n\")\n","\n","t2 = torch.cat([tensor, tensor, tensor], dim=0)\n","print(f\"Concatenated tensor (dim=0): \\n {t2} \\n\")\n","\n","# Squeeze and Unsqueeze operations\n","tensor_with_ones = torch.tensor([[1.0], [2.0], [3.0]])\n","print(f\"Tensor with ones shape: \\n {tensor_with_ones.shape} \\n\")\n","squeezed_tensor = tensor_with_ones.squeeze()\n","print(f\"Squeezed tensor shape: \\n {squeezed_tensor.shape} \\n\")\n","\n","unsqueezed_tensor = tensor.unsqueeze(0)\n","print(f\"Unsqueezed tensor shape at dimension 0 (shape): \\n {unsqueezed_tensor.shape} \\n\")\n","\n","# Add a dimension of size 1 at position 1\n","unsqueezed_tensor_1 = tensor.unsqueeze(1)\n","print(f\"Unsqueezed tensor at dimension 1 (shape): \\n {unsqueezed_tensor_1.shape} \\n\")"]},{"cell_type":"markdown","metadata":{"id":"GbZdexc4b7fU"},"source":["### 2. Neural networks\n","\n","Neural networks comprise of layers/modules that perform operations on data. Neural networks in PyTorch subclasses the nn.Module. A neural network is a module itself that consists of other modules (layers).\n","\n","To create a neural network in PyTorch, follow these steps:\n","1. Define a class that inherits from `nn.Module`.\n","2. Implement the `__init__` method to initialize the layers of the network.\n","3. Implement the `forward` method to define the forward pass of the network.\n","\n","\n","The `__init__` function is used to initialize the layers of the neural network. In this example, we will define an MLP with one hidden layer.\n","\n","The `forward` takes the input data x and passes it through the first fully connected layer (fc1). This layer applies a linear transformation to the input data. The first layer is then passed through the ReLU activation function. he output of the ReLU activation is finally passed through the second fully connected layer (fc2).  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MDlwd1pBb7fU"},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class SimpleMLP(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size):\n","        super(SimpleMLP, self).__init__()\n","        self.fc1 = nn.Linear(input_size, hidden_size)  # First fully connected layer\n","        self.fc2 = nn.Linear(hidden_size, output_size)  # Second fully connected layer\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))  # Apply ReLU activation to the first layer\n","        x = self.fc2(x)  # Apply the second layer\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"QRcCphjPb7fU"},"source":["Our MLP class is now defined. We can create an instance this class and move it to the `device`:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jmJdatxSb7fU"},"outputs":[],"source":["# Define the MLP\n","input_size = 784\n","hidden_size = 256\n","output_size = 10\n","model = SimpleMLP(input_size, hidden_size, output_size)\n","model.to(device)\n","\n","# Print the model architecture\n","print(model)"]},{"cell_type":"markdown","metadata":{"id":"cRD5_Ticb7fV"},"source":["To use the model, we pass it the input data. This executes the model’s forward, along with some background operations. Do not call `model.forward()` directly!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vV8oN1Wzb7fV"},"outputs":[],"source":["X = torch.rand(1, 784, device=device)\n","logits = model(X)\n","pred_probab = nn.Softmax(dim=1)(logits)\n","y_pred = pred_probab.argmax(1)\n","print(f\"Predicted class: {y_pred.item()}\")"]},{"cell_type":"markdown","metadata":{"id":"raPWl-Clb7fV"},"source":["Let’s break down the layers in the model.\n","We consider an input tensor `x` of shape `[1, 784]` (1 for the batch size, 784 for the size).\n","\n","- nn.Linear\n","\n","The linear layer is a module that applies a linear transformation on the input using its stored weights and biases.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QOYgCUd8b7fV"},"outputs":[],"source":["x = torch.rand(1, 784, device=device)\n","\n","layer1 = nn.Linear(in_features=784, out_features=20, device=device)\n","hidden1 = layer1(x)\n","print(hidden1.size())"]},{"cell_type":"markdown","metadata":{"id":"vA7Pm_5Wb7fV"},"source":["- nn.ReLU\n","\n","Non-linear activations are what create the complex mappings between the model’s inputs and outputs. They are applied after linear transformations to introduce nonlinearity, helping neural networks learn a wide variety of phenomena.\n","\n","In this model, we use nn.ReLU between our linear layers."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VEgcYALvb7fW"},"outputs":[],"source":["print(f\"Before ReLU: {hidden1}\\n\\n\")\n","hidden1 = nn.ReLU()(hidden1)\n","print(f\"After ReLU: {hidden1}\")"]},{"cell_type":"markdown","metadata":{"id":"-498GLiMb7fW"},"source":["- nn.Sequential\n","\n","nn.Sequential is an ordered container of modules. The data is passed through all the modules in the same order as defined. You can use sequential containers to put together a quick network like seq_modules."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OefE1rMqb7fW"},"outputs":[],"source":["seq_modules = nn.Sequential(\n","    layer1,\n","    nn.ReLU(),\n","    nn.Linear(20, 10, device=device)\n",")\n","logits = seq_modules(x)"]},{"cell_type":"markdown","metadata":{"id":"OwKCrUYsb7fW"},"source":["- nn.Softmax\n","\n","The last linear layer of the neural network returns logits - raw values in [-infty, infty] - which are passed to the nn.Softmax module. The logits are scaled to values [0, 1] representing the model’s predicted probabilities for each class. dim parameter indicates the dimension along which the values must sum to 1."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2G4l__dJb7fW"},"outputs":[],"source":["softmax = nn.Softmax(dim=1)\n","pred_probab = softmax(logits)\n","print(f\"Predicted probabilities: \\n {pred_probab} \\n\")"]},{"cell_type":"markdown","metadata":{"id":"ca3nQkt8b7fW"},"source":["Many layers inside a neural network are parameterized, i.e. have associated weights and biases that are optimized during training. You can access to the parameters of a model using: `parameters()` or `named_parameters()` methods."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OaGUwH6Rb7fW"},"outputs":[],"source":["print(f\"Model structure: {model}\\n\\n\")\n","\n","for name, param in model.named_parameters():\n","    print(f\"Layer: {name} | Size: {param.size()} \\n\")"]},{"cell_type":"markdown","metadata":{"id":"CNSW8a4Hb7fW"},"source":["### 3. Differentiation and gradients in PyTorch\n","\n","When training neural networks, the most frequently used algorithm is back propagation. In this algorithm, parameters (model weights) are adjusted according to the gradient of the loss function with respect to the given parameter.\n","\n","To compute those gradients, PyTorch has a built-in differentiation engine called torch.autograd. It supports automatic computation of gradient for any computational graph.\n","\n","To compute the derivative (of a variable, a loss, etc. with respect to a parameters), we use the `.backward()` method:  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WkA7vbKXb7fW"},"outputs":[],"source":["x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n","\n","y = x ** 2\n","z = y.sum()\n","\n","# Compute the gradient\n","z.backward()\n","\n","print(\"Gradient of z with respect to x:\", x.grad)"]},{"cell_type":"markdown","metadata":{"id":"izfTRgeJb7fW"},"source":["Consider the simplest one-layer neural network `z = w x + b`, with input x, parameters w and b, and some loss function.\n","\n","In this network, w and b are parameters, which we need to optimize. Thus, we need to be able to compute the gradients of loss function with respect to those variables."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l_wmTj_yb7fW"},"outputs":[],"source":["x = torch.ones(5)  # input tensor\n","y = torch.zeros(3)  # expected output\n","w = torch.randn(5, 3, requires_grad=True)\n","b = torch.randn(3, requires_grad=True)\n","z = torch.matmul(x, w)+b\n","loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n","\n","loss.backward()\n","print(f\"Gradient of the loss with respect to w: \\n {w.grad} \\n\")\n","print(f\"Gradient of the loss with respect to b: \\n {b.grad} \\n\")"]},{"cell_type":"markdown","metadata":{"id":"BN6l1jEfb7fW"},"source":["A function that we apply to tensors to construct computational graph is in fact an object of class Function. This object knows how to compute the function in the forward direction, and also how to compute its derivative during the backward propagation step. A reference to the backward propagation function is stored in grad_fn property of a tensor."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZQZ0fDLkb7fW"},"outputs":[],"source":["print(f\"Gradient function for z = {z.grad_fn}\")\n","print(f\"Gradient function for loss = {loss.grad_fn}\")"]},{"cell_type":"markdown","metadata":{"id":"Mi5PBFdqb7fW"},"source":["### 4. Training a neural network\n","\n","Training a neural network in PyTorch involves several key steps: defining the model, preparing the data, specifying the loss function and optimizer, and iterating through the training loop. Let's go through each step in detail.\n","\n","1. Define the model\n","\n","We will use the Multi-Layer Perceptron defined in the previous section.\n","\n","2. Prepare the data\n","\n","Next, we need to prepare our training and evaluation data. PyTorch provides `Dataset` and `DataLoader` classes to handle data loading and preprocessing efficiently. The `Dataset` class is an abstract class representing a dataset, and the `DataLoader` class provides an iterable over the given dataset. The `torchvision.datasets` module contains Dataset objects for many real-world vision data like CIFAR or COCO.\n","\n","In this example, we will use the [MNIST](https://yann.lecun.com/exdb/mnist/) dataset.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"f_1y-_hjb7fX"},"source":["PyTorch provides `Dataset` and `DataLoader` classes to handle data loading and preprocessing efficiently. The `Dataset` class is an abstract class representing a dataset, and the `DataLoader` class provides an iterable over the given dataset.\n","\n","In this section, we will work with the MNIST Dataset. The `torchvision.datasets` module contains Dataset objects for many real-world vision data like CIFAR or COCO."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L8ScjzPbb7fX"},"outputs":[],"source":["from torchvision import datasets\n","import torchvision.transforms as T\n","\n","training_data = datasets.MNIST(\n","    root=\"data\",\n","    train=True,\n","    download=True,\n","    transform=T.Compose([T.ToTensor(), T.Lambda(lambda x: torch.flatten(x))]),\n",")\n","\n","test_data = datasets.MNIST(\n","    root=\"data\",\n","    train=False,\n","    download=True,\n","    transform=T.Compose([T.ToTensor(), T.Lambda(lambda x: torch.flatten(x))]),\n",")"]},{"cell_type":"markdown","metadata":{"id":"oC9Q5Jg4b7fX"},"source":["We then pass the Dataset as an argument to DataLoader. The DataLoader class supports batching, shuffling, and loading data in parallel using multiprocessing workers. Here we define a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features and labels."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R3M726F1b7fX"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","batch_size = 64\n","\n","# Create data loaders.\n","train_dataloader = DataLoader(training_data, batch_size=batch_size)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size)\n","\n","for X, y in test_dataloader:\n","    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n","    print(f\"Shape of y: {y.shape} {y.dtype}\")\n","    break"]},{"cell_type":"markdown","metadata":{"id":"9MydF3fqb7fX"},"source":["3. Specify the Loss Function and Optimizer\n","\n","The loss function measures the difference between the predicted outputs and the true labels. The optimizer updates the model parameters to minimize the loss."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pUiI6jZDb7fX"},"outputs":[],"source":["import torch.optim as optim\n","\n","# Define the loss function\n","criterion = nn.CrossEntropyLoss()\n","\n","# Define the optimizer\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"]},{"cell_type":"markdown","metadata":{"id":"IPgAiBsGb7fX"},"source":["4. Training Loop\n","\n","The training loop involves iterating over the dataset, performing forward and backward passes, and updating the model parameters. The main steps are:\n","- Zero the Parameter Gradients: `optimizer.zero_grad()` clears the old gradients from the last step. This is important because PyTorch accumulates gradients by default.\n","- Forward Pass: The input data is passed through the model to get the outputs. The loss is then computed using the criterion.\n","- Backward Pass: `loss.backward()` computes the gradient of the loss with respect to the model parameters.\n","- Optimization: `optimizer.step()` updates the model parameters using the computed gradients."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"61GkFn0yb7fY"},"outputs":[],"source":["# Training loop\n","num_epochs = 10\n","for epoch in range(num_epochs):\n","    model.train()  # Set the model to training mode\n","    running_loss = 0.0\n","    for i, data in enumerate(train_dataloader, 0):\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        # Zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward pass and optimization\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Print statistics\n","        running_loss += loss.item()\n","        if i % 100 == 99:  # Print every 100 mini-batches\n","            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_dataloader)}], Loss: {running_loss/100:.4f}')\n","            running_loss = 0.0\n","\n","print(\"Training complete.\")\n"]},{"cell_type":"markdown","metadata":{"id":"GFA-jRaub7fY"},"source":["5. Evaluation\n","\n","After training, you can evaluate the model on the test dataset to see how well it performs. We need to:\n","- Set the Model to Evaluation Mode: `model.eval()` sets the model to evaluation model\n","- Disable Gradient Calculation: `torch.no_grad()` disables gradient calculation. It also reduce memory consumption and speeds up computation.\n","- Compute Accuracy: The model's predictions are compared to the true labels to compute the accuracy.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X1lUw0m2b7fY"},"outputs":[],"source":["# Evaluation loop\n","model.eval()  # Set the model to evaluation mode\n","correct = 0\n","total = 0\n","with torch.no_grad():  # Disable gradient calculation\n","    for data in test_dataloader:\n","        images, labels = data\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')\n"]},{"cell_type":"markdown","metadata":{"id":"ywpy7Rm3b7fY"},"source":["## Third part: CNN, ResNet, and deep learning architecture for Image Classification\n","\n","In this part, we will see some classic architectures of convolutional networks and how to use pre-trained networks to speed-up training on new tasks."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4aiEmAJub7fY"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision.datasets import FashionMNIST\n","from torchvision import transforms\n","from matplotlib import pyplot as plt\n","from functools import partial\n","from tqdm import tqdm\n","import math\n","import numpy as np\n","import matplotlib.ticker as ticker\n","\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"1KKgO0-eb7fZ"},"source":["# Importance of weight initialization\n","\n","The initialization of the weights of a Neural Network are a key aspect of the training of Neural Networks."]},{"cell_type":"markdown","metadata":{"id":"4swdX34tb7fZ"},"source":["\n","\n","## Question 1\n","Using the FashionMNIST dataset, train a feed forward neural network with 2 hidden layers, with sizes 64 and 32. Use a cross-entropy loss.\n","\n","Make sure to enable the initialization of the weights and biases with different methods. You will study the constante, kaiming, uniform, normal and xavier initializations (see `torch.nn.init`)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1_37Wl9Sb7fZ"},"outputs":[],"source":["train_transforms = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(0.286, 0.353),\n","    transforms.Lambda(lambda x: x.view(-1))\n","])\n","\n","train_dataset = FashionMNIST(root='./data', train=True, download=True, transform=train_transforms)\n","test_dataset = FashionMNIST(root='./data', train=False, download=True, transform=train_transforms)\n","\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n","\n","class FFN(nn.Module):\n","    def __init__(self, input_size, output_size, init=nn.init.xavier_normal_):\n","        ### YOUR CODE HERE\n"]},{"cell_type":"markdown","metadata":{"id":"EOBciRTbb7fZ"},"source":["\n","## Question 2\n","Plot the different training losses and compare the different initializations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EtK8sP58b7fa"},"outputs":[],"source":["\n","def train(model, train_loader, optimizer, criterion, device, epochs=10):\n","    model.train()\n","    train_losses = []\n","    for epoch in range(epochs):\n","        for batch_idx, (data, target) in enumerate(train_loader):\n","            data, target = data.to(device), target.to(device)\n","            optimizer.zero_grad()\n","            output = model(data)\n","            loss = criterion(output, target)\n","            train_losses.append(loss.detach().item())\n","            loss.backward()\n","            optimizer.step()\n","    return train_losses\n","\n","def train_and_plot_all_inits(train_loader, criterion, device, inits, epochs=1, lr=3e-3):\n","    ### YOUR CODE HERE\n","\n","\n","criterion = nn.CrossEntropyLoss()\n","inits = {\n","    ### YOUR CODE HERE\n","}\n","train_and_plot_all_inits(train_loader, criterion, device, inits)"]},{"cell_type":"markdown","metadata":{"id":"1mSndvJSb7fa"},"source":["# Regularization\n","\n","When training a Neural Network, we want to avoid at all cost overfitting. Overfitting occurs when when have overlearned on our training set. The first thing to do is to compare the performances (loss or metrics) of the train dataset with the validation dataset. If you see a gap in performances, this means that you have overfitted your network.\n","\n","To study the impact of overfitting, we will leverage a synthetic dataset given by the following Dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tZYSkUR6b7fb"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","from sklearn.datasets import make_circles\n","import matplotlib.pyplot as plt\n","\n","class CircleDataset(nn.Module):\n","    def __init__(self, num_samples, std=0.1, transform=None, seed=None):\n","        self.x, self.y = make_circles(n_samples=num_samples, noise=std, random_state=seed)\n","        self.x = torch.from_numpy(self.x).float()\n","        self.y = torch.from_numpy(self.y).float()\n","        self.len = self.x.shape[0]\n","        self.transform = transform\n","\n","\n","    def __getitem__(self, index):\n","        if self.transform:\n","            return self.transform(self.x[index]), self.y[index]\n","        return self.x[index], self.y[index]\n","\n","    def __len__(self):\n","        return self.len\n","\n","# Plot the dataset\n","dataset = CircleDataset(100)\n","plt.scatter(dataset.x[:,0], dataset.x[:,1], c=dataset.y)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"MU8Tfb4Xb7fb"},"source":["### Question 3\n","Create a CircleDataset validation and test set with 500 samples, std=0.1. To be able to compare between experiments, use seed 3407 for the validation set and 3408 for the test set. This test set must be the same for every experiment we will do.\n","\n","Note: Validation and test set are crucial to evaluate the performance of an ML algorithm. The goal is to be able to evaluate the generalisation abilities of our method. Indeed, if we overfit on the train set, we will have optimal performances but our method will not work in out of train set data.\n","\n","The validation set is used for both monitoring the evolution of training and to tune hyperparameters.\n","\n","The test set must be used ONLY at the end of training/hparameters tuning. Otherwise, you risk overfitting on it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GucPCS-nb7fb"},"outputs":[],"source":["### YOUR CODE HERE\n","circle_val_loader = ### YOUR CODE HERE\n","\n","### YOUR CODE HERE\n","circle_test_loader = ### YOUR CODE HERE"]},{"cell_type":"markdown","metadata":{"id":"z2IINnJtb7fc"},"source":["### Question 4\n","Create a Feed Forward architecture with 2 hidden layers of size 500, 500. Use the `BCELoss`, and the Adam optimizer with lr=5e-3. Train for 50000 steps ploting the loss and the accuracy for both the train and val set after each epoch end. Also, compute the accuracy at the end of training on the test set. Train on a new Circle dataset having 50 data points. What can you say about the result?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ht0so3xlb7fc"},"outputs":[],"source":["def train(train_loader, val_loader, test_loader, device, epochs=1000):\n","    ### YOUR CODE HERE\n","    return train_losses, train_accs, val_losses, val_accs, test_acc\n","\n","train_circle_dataset = CircleDataset(50, std=0.1, seed=None)\n","train_circle_loader = torch.utils.data.DataLoader(dataset=train_circle_dataset, batch_size=64, shuffle=True)\n","\n","train_losses, train_accs, val_losses, val_accs, test_acc = train(train_circle_loader, circle_val_loader, circle_test_loader, device, epochs=1000)\n","\n","# Plot the training and validation loss\n","plt.figure(figsize=(10, 5))\n","plt.plot(train_losses, label=\"train\")\n","plt.plot(val_losses, label=\"val\")\n","plt.legend()\n","plt.show()\n","\n","# Plot the training and validation accuracy\n","plt.figure(figsize=(10, 5))\n","plt.plot(train_accs, label=\"train\")\n","plt.plot(val_accs, label=\"val\")\n","plt.legend()\n","plt.show()\n","\n","print(f\"Test accuracy: {test_acc:.2f}\")"]},{"cell_type":"markdown","metadata":{"id":"cJrfiPBjb7fc"},"source":["## Influence of the amount of data\n","The most important regularization technique is more data.\n","### Question 5\n","Create multiple train sets having [10, 100, 1000, 10000] datapoints and train the network on this train sets. Plot for each run the training and val losses and accuracy and the test accuracy. How does the overfitting evolves?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TaD-7iKNb7fc"},"outputs":[],"source":["train_set_size = [10, 100, 1000, 10000]\n","train_losses = {}\n","val_losses = {}\n","train_accs = {}\n","val_accs = {}\n","test_accs = {}\n","### YOUR CODE HERE\n"]},{"cell_type":"markdown","metadata":{"id":"u3o00iPAb7fc"},"source":["## Data augmentation\n","\n","In real life, we often have to deal with a small dataset. In this case, we can use data augmentation to artificially increase the size of the dataset. For images multiple data augmentations techniques exist such as:\n","- Cropping\n","- Color Jittering\n","- Geometric transformations\n","- A lot other techniques (see `torchvision.Transforms` or `Albumentation`)\n","\n","### Question 6\n","The main idea here is to \"create\" new data from existing datapoints.\n","Create a lambda transform that jitters the datapoints with Gaussian noise.\n","Use a dataset with 50 samples and train a classifier that leverage the augmentation with stds = [0, 1e-2, 1e-1, 2e-1, 1]. Make sure to have the same train dataset for each exp.\n","Discuss the results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kGxpzLyXb7fc"},"outputs":[],"source":["class RandomNoiseTransform(object):\n","    def __init__(self, std=0.001):\n","        ### YOUR CODE HERE\n","\n","noise_levels = [0, 1e-2, 1e-1, 2e-1, 1]\n","num_noise_levels = len(noise_levels)\n","train_losses = {}\n","val_losses = {}\n","train_accs = {}\n","val_accs = {}\n","test_accs = {}\n","\n","### YOUR CODE HERE"]},{"cell_type":"markdown","metadata":{"id":"12aHnpOlb7fd"},"source":["### Question 7\n","Visualize the augmented training set for the noise level that gave the best test accuracy. Plot the original points in one color and the augmented one in another. To be able to visualize well what's happening, display 100 iterations on the augmented dataset. What conclusion can you draw from this observation. Could you think of a \"smarter\" way to augment the data if you can make assumption on the structure of the data manifold?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nGh_Y0ocb7fd"},"outputs":[],"source":["### YOUR CODE HERE"]},{"cell_type":"markdown","metadata":{"id":"Nt2eQZKgb7fd"},"source":["### Question 8\n","Another way to regularize the training is to use dropout. Implement the dropout operation as an nn.Module. Remember, dropout is only implemented when training. Fortunally, `torch.nn.Module` has a flag `training` which is true if the model is training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LYO1xWfqb7fd"},"outputs":[],"source":["class Dropout(nn.Module):\n","   ### YOUR CODE HERE"]},{"cell_type":"markdown","metadata":{"id":"ht-8iE08b7fd"},"source":["### Question 9\n","Use dropout to train a model on the circle dataset with 100 samples. Use the same architecture as in Question 8. train with different values of dropout and plot the training and validation loss and accuracy. Compute the test accuracy. What can you conclude?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uwXg4jUfb7fd"},"outputs":[],"source":["def train_dropout(dropout_rate, train_loader, val_loader, test_loader, device, epochs=1000):\n","    ### YOUR CODE HERE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rn1eRIm7b7fd"},"outputs":[],"source":["dropout_rates = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n","\n","train_losses = {}\n","val_losses = {}\n","train_accs = {}\n","val_accs = {}\n","test_accs = {}\n","### YOUR CODE HERE\n"]},{"cell_type":"markdown","metadata":{"id":"g9BLNj1eSk7z"},"source":["# ImageNet\n","\n","The [ImageNet](http://www.image-net.org/) dataset is one of the main datasets used in image recognition. It contains more than 14 million images annotated according to the hierarchical structure of the [WordNet](https://wordnet.princeton.edu/) dataset.\n","Images are organized into classes and subclasses having semantic relationships, for example 'car' and 'plane' are 'vehicles', 'cat' and 'bird' are 'animals', 'plane' and 'bird' are 'flying objects', etc ...\n","\n","ImageNet was accompanied by the challenge: 'Large Scale Visual Recognition Challenge' (ILSVRC), each edition offers different challenges (classification, detection, segmentation, ...) based on a subset of the dataset.\n","\n","The advantage of having large datasets for learning is to be able to train networks on fairly general tasks, and after reuse the  learned weights for other applications. This operation is called transfer learning. A deep neural network (DNN) learns more and more abstract (hence high-level) features as one progresses through the layers.\n","\n","Thus, a neural network (NN) pre-trained on a large dataset has low-level characteristics (learned in the first layers) potentially transferable to many tasks. These include texture, color, etc.  An immediate advantage is saving time as one does not have to re-train the NN for every new task from scratch. Another advantage is the fact that the models obtained are more robust. Indeed, a network pre-trained on a complete, large-scale dataset needs on the one hand fewer examples (since it has already seen a lot of them during the first training), and has less risk of over-learning the low level characteristics."]},{"cell_type":"markdown","metadata":{"id":"JSrcn00fTgPO"},"source":["# AlexNet\n","\n","In 2012, the [AlexNet](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) architecture won the ImageNet competition. The name comes from the first author Alex Krizhevsky. It is a CNN that classifies images into 1000 categories by producing a probability distribution over 1000 classes for each image. The metric used is the percentage of error among the *k* most probable classes (*top-k* error), i.e., we look to see if the true/correct prediction is among the *k* most probable classes predicted by the algorithm.\n","AlexNet got a top-5 error of 15.3% versus 26.2% for the second best result. This remarkable success led to the adoption of DNNs for the last decade.\n","\n","Recall that a convolution takes as input a tensor of rank 3 whose first two axes are indexed according to the coordinates of the pixels ($W$, $H$ for width and height) and the last axis stores the channels (for RGB images this is 3). The convolutional kernel is a rank $4$ tensor whose first two axes are indexed according to the coordinates of the pixels supporting the kernel and the last two axes store a matrix of size $d_{in} \\times d_{out}$ , where $d_{in}$ is the dimension of the features before convolution, and $d_{out}$ the dimension after the convolution, which will be applied to the input.\n","\n","The AlexNet architecture is illustrated in the following diagram:"]},{"cell_type":"markdown","metadata":{"id":"bT4nHAOgXK1o"},"source":["![](https://drive.google.com/uc?export=view&id=1qXGfYOJRU0pgCcGydat0u2Y9csso9nIQ)"]},{"cell_type":"markdown","metadata":{"id":"XSxYxLJDXS_4"},"source":["Images are represented by volumes whose height and width are the dimensions of the image and the depth is the number of channels. The size of the convolutional kernel is indicated by the small squares. For instance, we see that the input image is an RGB image of size 224 by 224 and that the first filter has a size of 11 by 11, its input dimension is 3 and its output dimension is 64."]},{"cell_type":"markdown","metadata":{"id":"6k6KkOxnXjJA"},"source":["### Question 10:\n","Implement the AlexNet architecture.\n","You will complete the following code:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GTI5jeOV_4PU"},"outputs":[],"source":["import torch.nn as nn\n","import torch.utils.model_zoo as model_zoo\n","\n","class AlexNet(nn.Module):\n","\n","    def __init__(self, num_classes=1000):\n","        super(AlexNet, self).__init__()\n","        self.features = nn.Sequential(\n","            # TODO\n","            # ...\n","\t\t)\n","        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n","        self.classifier = nn.Sequential(\n","            nn.Dropout(),\n","            nn.Linear(256 * 6 * 6, 4096),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(4096, num_classes),\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.avgpool(x)\n","        x = x.view(x.size(0), 256 * 6 * 6)\n","        x = self.classifier(x)\n","        return x\n"]},{"cell_type":"markdown","metadata":{"id":"9gA2qpm2Xvsh"},"source":["Note that some information does not appear on the diagram:\n","* the activations are ReLu\n","* every convolution or fully connected layer is followed by an activation.\n","\n","Recall the convolution and pooling operations: for an image of size *h* with a filter of size *k*, padding of *p* and stride of *s* the output size is:\n","\n","$$ \\frac{h-k+2p}{s} + 1 $$\n","\n","In AlexNet the convolutional layers have a stride of 1 (except the 1st) and the pooling layers have a 'kernel' of size 3 and stride 2.\n","\n","Finally, recall the adaptive average pooling function (that performs adaptive pooling), which renders an \"image\" of predefined size (here 6 by 6). You need to deduce the missing parameters to implement the network described in the diagram above."]},{"cell_type":"markdown","metadata":{"id":"SOrzbkG5ZAuj"},"source":["# Transfer Learning\n","\n","Note that the above AlexNet architecture has two distinct parts:  \n","* a first \"features\" sub-network, responsible for extracting relevant characteristics from the image, and\n","* a \"classifier\" that is applied on top (i.e., the fully connected layers)\n","\n","The \"features\" part is reusable for *other* tasks. We will import a pre-trained model on ImageNet for the AlexNet architecture. We will then use the corresponding \"features\" for another classification problem.\n","\n","We will use the following script defining a \"classifier\" and applying it to AlexNet features, pre-trained or not."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bcu21L1EZWcb"},"outputs":[],"source":["model_urls = {\n","'alexnet': 'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth',\n","}\n","\n","def alexnet_classifier(num_classes):\n","    classifier = nn.Sequential(\n","        nn.Dropout(),\n","        nn.Linear(256 * 6 * 6, 128),\n","        nn.ReLU(inplace=True),\n","        nn.Dropout(),\n","        nn.Linear(128, 64),\n","        nn.ReLU(inplace=True),\n","        nn.Linear(64, num_classes),\n","    )\n","    return classifier\n","\n","\n","def alexnet(num_classes, pretrained=False, **kwargs):\n","    \"\"\"AlexNet model architecture from the \"One weird trick...\"\n","    <https://arxiv.org/abs/1404.5997> paper.\n","\n","    Args:\n","    pretrained (bool): If True, returns a model pre-trained on ImageNet\n","    \"\"\"\n","    model = AlexNet(**kwargs)\n","    if pretrained:\n","        model.load_state_dict(model_zoo.load_url(model_urls['alexnet']))\n","        for p in model.features.parameters():\n","            p.requires_grad=False\n","    classifier = alexnet_classifier(num_classes)\n","    model.classifier = classifier\n","\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"omGIKMecZU9t"},"source":["We will compare the pre-trained model to a non-pretrained model on the CIFAR-10 dataset. This is a much smaller dataset than ImageNet but still very useful for evaluating models while avoiding long training times. It contains 60,000 images (50,000 training, 10,000 test) of size 32 by 32 split into 10 classes. Note that gradient descent has been disabled for pre-trained feature weights to avoid corrupting them during training.\n","\n","We will load the dataset with the following script:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VjZFaOgH5UX2"},"outputs":[],"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data.sampler import SubsetRandomSampler\n","\n","torchvision.transforms.functional.resize\n","transform = transforms.Compose(\n","    [\n","     transforms.Resize(size=(224, 224)),\n","     transforms.ToTensor(),\n","     transforms.Normalize((0.5,), (0.5,)),\n","])\n","\n","\n","batch_size = 64\n","\n","idx_train = np.arange(50000)\n","np.random.shuffle(idx_train)\n","idx_train = idx_train[:1000]\n","\n","trainset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n","trainloader = torch.utils.data.DataLoader(trainset,batch_size=batch_size,shuffle=False,num_workers=2,\n","                                         sampler=SubsetRandomSampler(idx_train))\n","\n","idx_test = np.arange(10000)\n","np.random.shuffle(idx_test)\n","idx_test = idx_test[:1000]\n","\n","testset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, transform=transform, download=True)\n","testloader = torch.utils.data.DataLoader(trainset,batch_size=batch_size,shuffle=False,num_workers=2)\n","\n","\n","def imshow(img):\n","    img = img / 2 + 0.5     # unnormalize\n","    npimg = img.numpy()\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","    plt.show()\n","\n","\n","# get some random training images\n","dataiter = iter(trainloader)\n","images, labels = dataiter.next()\n","\n","# show images\n","imshow(torchvision.utils.make_grid(images))"]},{"cell_type":"markdown","metadata":{"id":"xNKmhIHS5XWw"},"source":["You will be able to display the filters of the first convolutional layer, and compare these filters for the pre-trained network and the one trained on CIFAR-10, by viewing them with the following script:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NfFmG4k-7f2o"},"outputs":[],"source":["def imshow_filters(img):\n","    npimg = img.numpy()\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","    plt.show()\n","\n","def show_weights(MyModel):\n","  \"\"\"Displays the filters of the 1st convolutional layer\n","  of the input model\n","  Input:\n","  - MyModel: the input model\n","  \"\"\"\n","  ii = 0\n","  filter = MyModel.features[ii].weight.cpu().data\n","  #Normalizing the values to [0,1]\n","  f_min, f_max = filter.min(), filter.max()\n","  filter = (filter - f_min) / (f_max - f_min)\n","  print(\"The filter shape is {}\".format(filter.shape))\n","  imshow_filters(torchvision.utils.make_grid(filter))\n"]},{"cell_type":"markdown","metadata":{"id":"JHoMF89L7gex"},"source":["We have intentionally reduced the size of the images to speed up training. Note that AlexNet being is designed for images of size 224 by 224, we apply a scaling transformation (by the bilinear interpolation method, seen in TD2).\n","\n","In the following, we will use the following training loop:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3m2og07XC5w3"},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","\n","def accuracy(net, test_loader, cuda=True):\n","  net.eval()\n","  correct = 0\n","  total = 0\n","  loss = 0\n","  with torch.no_grad():\n","      for data in test_loader:\n","          images, labels = data\n","          if cuda:\n","            images = images.type(torch.cuda.FloatTensor)\n","            labels = labels.type(torch.cuda.LongTensor)\n","          outputs = net(images)\n","\n","          _, predicted = torch.max(outputs.data, 1)\n","          total += labels.size(0)\n","          correct += (predicted == labels).sum().item()\n","\n","  net.train()\n","  print('Accuracy of the network on the test images: %d %%' % (\n","      100 * correct / total))\n","\n","  return 100.0 * correct"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ebzl4zvd7ztY"},"outputs":[],"source":["def train(net, optimizer, train_loader, test_loader, loss,  n_epoch = 5,\n","          train_acc_period = 100,\n","          test_acc_period = 5,\n","          cuda=True):\n","  loss_train = []\n","  loss_test = []\n","  total = 0\n","  for epoch in range(n_epoch):  # loop over the dataset multiple times\n","      running_loss = 0.0\n","      running_acc = 0.0\n","      for i, data in enumerate(train_loader, 0):\n","\n","          # get the inputs\n","          inputs, labels = data\n","          if cuda:\n","            inputs = inputs.type(torch.cuda.FloatTensor)\n","            labels = labels.type(torch.cuda.LongTensor)\n","          # print(inputs.shape)\n","\n","          # zero the parameter gradients\n","          optimizer.zero_grad()\n","\n","          outputs = net(inputs)\n","\n","          loss = criterion(outputs, labels)\n","          loss.backward()\n","          optimizer.step()\n","          total += labels.size(0)\n","\n","          # print statistics\n","          running_loss = 0.33*loss.item()/labels.size(0) + 0.66*running_loss\n","          _, predicted = torch.max(outputs.data, 1)\n","          correct = (predicted == labels).sum().item()/labels.size(0)\n","          running_acc = 0.3*correct + 0.66*running_acc\n","          if i % train_acc_period == train_acc_period-1:\n","            print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss))\n","            print('[%d, %5d] acc: %.3f' %(epoch + 1, i + 1, running_acc))\n","            running_loss = 0.0\n","            total = 0\n","\n","      if epoch % test_acc_period == test_acc_period-1:\n","          cur_acc, cur_loss = accuracy(net, test_loader, cuda=cuda)\n","          print('[%d] loss: %.3f' %(epoch + 1, cur_loss))\n","          print('[%d] acc: %.3f' %(epoch + 1, cur_acc))\n","\n","  print('Finished Training')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2jhH-I8kb7fg"},"outputs":[],"source":["###"]},{"cell_type":"markdown","metadata":{"id":"sBMUCBIQ70eU"},"source":["### Question 11\n","Run the following code and compare the performance between (i) the model and (ii) its pre-trained version. Specifically, compare the filters of the first convolutional layers."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z5Irc6V3lGBB"},"outputs":[],"source":["# This is the main part where you run the training and test loops and compute accuracy\n","net = alexnet(num_classes=10, pretrained=False)\n","\n","use_cuda = True\n","if use_cuda and torch.cuda.is_available():\n","    print(\"using cuda\")\n","    net.cuda()\n","learning_rate = 1e-3\n","optimizer = torch.optim.Adam(net.parameters(),lr=learning_rate)\n","train(net, optimizer, trainloader, testloader, criterion,  n_epoch = 50,\n","      train_acc_period = 10,\n","      test_acc_period = 1000)\n","show_weights(net)\n","accuracy(net, testloader, cuda=use_cuda)"]},{"cell_type":"markdown","metadata":{"id":"ZrvF_Jtg825D"},"source":["# VGG-Net\n","\n","The VGG-Net architecture developed by the Visual Geometry Group team at the University of Oxford won second place in the ImageNet 2014 challenge. The variants of VGG-Net obtains up to 7.3% top-5 error on the ImageNet 2012 challenge vs 15.3% for AlexNet.\n","\n","The VGG-Net architecture is available in several variants presented in the following table:"]},{"cell_type":"markdown","metadata":{"id":"zCjzc0R-9Vnj"},"source":["![](https://drive.google.com/uc?export=view&id=1JB2rzZHiePoKlwqqeeHg-yTwIvMjw4-m)"]},{"cell_type":"markdown","metadata":{"id":"jigus5E59cxj"},"source":["In VGG-Net:\n","* All the convolutions use a kernel of size 3 by 3 with a padding of 1. The convolutions therefore *preserve* the size of the image.\n","* All the max pooling layers have a size of 2 by 2 and a stride of 2.\n","* An adaptive avg pooling layer is applied before classifying it so as to reduce the image to a size 7 by 7. This is then vectorized, then sent to the classifier.\n","\n","Note that the \"features\" part of VGG-Net can be stored in a list with a simple loop going through a list of parameters (here `cfg ['A']`).\n","\n","### Question 12\n","\n","You need to complete the `make_layers` function with the following code.\n","Note, if `batch_norm == True` we will need to add a batch norm layer between each convolutional layer and the following ReLu layer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C4Ad_7_q9vG_"},"outputs":[],"source":["import torch.nn as nn\n","import torch.utils.model_zoo as model_zoo\n","\n","\n","model_urls = {\n","    'vgg11': 'https://download.pytorch.org/models/vgg11-bbd30ac9.pth',\n","    'vgg11_bn': 'https://download.pytorch.org/models/vgg11_bn-6002323d.pth' # bn: batch normalization\n","}\n","\n","\n","class VGG(nn.Module):\n","\n","    def __init__(self, features, num_classes=1000, init_weights=True):\n","        super(VGG, self).__init__()\n","        self.features = features\n","        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n","        self.classifier = nn.Sequential(\n","            nn.Linear(512 * 7 * 7, 4096),\n","            nn.ReLU(True),\n","            nn.Dropout(),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(True),\n","            nn.Dropout(),\n","            nn.Linear(4096, num_classes),\n","        )\n","        if init_weights:\n","            self._initialize_weights()\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.avgpool(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.classifier(x)\n","        return x\n","\n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","                if m.bias is not None:\n","                    nn.init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.BatchNorm2d):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.Linear):\n","                nn.init.normal_(m.weight, 0, 0.01)\n","                nn.init.constant_(m.bias, 0)\n","\n","\n","def make_layers(cfg, batch_norm=False):\n","    layers = []\n","\t  # TODO\n","\n","    return nn.Sequential(*layers)\n","\n","\n","cfg = { # M stands for max pooling\n","    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n","    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n","    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n","}"]},{"cell_type":"markdown","metadata":{"id":"uKiObfgJ92uX"},"source":["### Question 13\n","As you did with AlexNet, evaluate (using the following functions) version A of VGG-Net (i) with and (ii) without pre-training on ImageNet.\n","Use `n_epoch = 15`.\n","\n","Bonus: do the same for version E."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jyl1IYZl9909"},"outputs":[],"source":["def vgg_11_classifier(num_classes):\n","  classifier = nn.Sequential(\n","            nn.Dropout(),\n","            nn.Linear(512 * 7 * 7, 128),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(),\n","            nn.Linear(128, 64),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(64, num_classes),\n","        )\n","  return classifier\n","\n","def vgg11_bn(num_classes, pretrained=False, **kwargs):\n","    \"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","    \"\"\"\n","    if pretrained:\n","        kwargs['init_weights'] = False\n","    model = VGG(make_layers(cfg['A'], batch_norm=True), **kwargs) # change cfg version for bonus\n","    if pretrained:\n","        model.load_state_dict(model_zoo.load_url(model_urls['vgg11_bn'])) # change model url for bn\n","    model.classifier = vgg_11_classifier(num_classes)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W913W1Ovb7fh"},"outputs":[],"source":["# Todo"]},{"cell_type":"markdown","metadata":{"id":"dRj5kY51V7CW"},"source":["# ResNet\n","\n","The ResNet architecture developed at Microsoft won the ImageNet 2015 challenge with a top-5 error of 3.57%. Rather than using a big convolutional kernel, the idea behind ResNet is replacing it with several successive layers using small kernels, introducing more non-linearity and depth into the model. This nevertheless poses a problem for gradient descent, and in particular increases the risk of saturation of the gradients due to the numerous ReLu activations. The simple (yet effective) idea behind ResNet is to use connections that bypass nonlinearities, so the gradient can easily 'go down' by taking these short circuits. One way to interpret this idea is to let the network self-regulate the amount of nonlinearities in its structure."]},{"cell_type":"markdown","metadata":{"id":"y-0NwAlXQT1q"},"source":["# Residual Block\n","\n","The figure below displays the Residual Block of ResNet.\n","\n","![](https://drive.google.com/uc?export=view&id=111dS4Trq3HdRb0-9BzzimEMDy4QAlZI9)\n","\n","\n","Denote the input by 𝐱. We assume that the desired underlying mapping we want to obtain by learning is 𝑓(𝐱), to be used as the input to the activation function on the top. On the left side of the figure, the portion within the dotted-line box must directly learn the mapping  𝑓(𝐱). On the right, the portion within the dotted-line box needs to learn the residual mapping  𝑓(𝐱)−𝐱 , which is how the residual block derives its name. If the identity mapping  𝑓(𝐱)=𝐱  is the desired underlying mapping, the residual mapping is easier to learn: we only need to push the weights and biases of the upper weight layer (e.g., fully-connected layer and convolutional layer) within the dotted-line box to zero. The right diagram illustrates the residual block of ResNet, where the solid line carrying the layer input 𝐱 to the addition operator is called a Residual Connection (or shortcut connection). With residual blocks, inputs can forward propagate faster through the residual connections across layers."]},{"cell_type":"markdown","metadata":{"id":"Dkx2NpADSIgB"},"source":["ResNet follows VGG’s full 3×3 convolutional layer design.\n","* The residual block has two 3×3  convolutional layers with the same number of output channels. * Each convolutional layer is followed by a batch normalization layer and a ReLU activation function.\n","* Then, we skip these two convolution operations and add the input directly before the final ReLU activation function.\n","\n","This kind of design requires that the output of the two convolutional layers has to be of the same shape as the input, so that they can be added together. If we want to change the number of channels, we need to introduce an additional  1×1 convolutional layer to transform the input into the desired shape for the addition operation.\n","\n","### Question 14\n","Fill in the code below so that it generates two types of networks: (1) one where we add the input to the output before applying the ReLU nonlinearity whenever use_1x1conv=False, and (2) one where we adjust channels and resolution by means of a 1×1 convolution before adding.\n","This is displayed in the following figure:\n","\n","![](https://drive.google.com/uc?export=view&id=1iE0l_2hEiNLbk8bTOSjQTbqQJaN9jVGR)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kIHRlf8XSHD3"},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch.nn import functional as F\n","\n","class BasicBlock(nn.Module):\n","    \"\"\"The Residual block of ResNet.\"\"\"\n","    def __init__(self, input_channels, num_channels, use_1x1conv=False,\n","                 strides=1):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(input_channels, num_channels, kernel_size=3,\n","                               padding=1, stride=strides)\n","        # TODO\n","        # ...\n","\n","\n","    def forward(self, X):\n","        Y = F.relu(self.bn1(self.conv1(X)))\n","        # TODO\n","        # ...\n","        return F.relu(Y)"]},{"cell_type":"markdown","metadata":{"id":"0nM8wzufTKFc"},"source":["### Question 15\n","\n","For a random input:\n","`X = torch.rand(4, 3, 6, 6)`, create two `BasicBlock` blocks with (1) input and output of the same shape (use `input_channels=3` and `num_channels=3`), (2) halve (divided by 2) the output height and width while increasing the number of output channels.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7AMJ2ZCQu4kQ"},"outputs":[],"source":["X = torch.rand(4, 3, 6, 6)\n","residual_block1 = BasicBlock(3, 3, use_1x1conv= # TODO)\n","Y1 = residual_block1(X)\n","residual_block2 = BasicBlock(3, 6, use_1x1conv=# TODO, strides=# TODO)\n","Y2 = residual_block2(X)\n","print(\"Shape of first block is {}, shape of second block is {}\".format(Y1.shape, Y2.shape))"]},{"cell_type":"markdown","metadata":{"id":"41BfERA8V0un"},"source":["# ResNet architecture\n","\n","The first layers of ResNet are\n","* a 7×7  convolutional layer with 64 output channels and a stride of 2 and padding 3, which is followed by\n","* a batch normalization layer\n","* a 3×3  max pooling layer with a stride of 2, padding 1.\n","\n","Note that in ResNet the batch normalization is added after each convolutional layer.\n","\n","### Question 16\n","\n","Fill in the code below:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O8JSxjHZWnrB"},"outputs":[],"source":["b1 = nn.Sequential( # TODO )"]},{"cell_type":"markdown","metadata":{"id":"OJxQcDS2W3d-"},"source":["## Modules\n","\n","ResNet uses four modules made up of residual blocks, each of which uses several residual blocks with the same number of output channels. The number of channels in the first module is the same as the number of input channels. Since a maximum pooling layer with a stride of 2 has already been used, it is not necessary to reduce the height and width. In the first residual block for each of the subsequent modules, the number of channels is doubled compared with that of the previous module, and the height and width are halved.\n","\n","### Question 17\n","\n","You need to implement this `_make_layer` module (code below). Note that you need to perfrom a special processing on the first module."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PGVpXOavXLN8"},"outputs":[],"source":["def _make_layer(input_channels, num_channels, num_residuals,\n","                 first_block=False):\n","    res_block = []\n","    for i in range(num_residuals):\n","        # TODO\n","        # ...\n","    return res_block"]},{"cell_type":"markdown","metadata":{"id":"aTzohbUwXb1j"},"source":["## Adding all modules to ResNet\n","\n","Now, we need to add all modules to ResNet. For this, you need the following code:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u6_f8iaPXwBr"},"outputs":[],"source":["b2 = nn.Sequential(*_make_layer(64, 64, 2, first_block=True))\n","b3 = nn.Sequential(*_make_layer(64, 128, 2))\n","b4 = nn.Sequential(*_make_layer(128, 256, 2))\n","b5 = nn.Sequential(*_make_layer(256, 512, 2))"]},{"cell_type":"markdown","metadata":{"id":"VL2QJ06SX1Pr"},"source":["Finally, we add a global average pooling layer, followed by the fully-connected layer output:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KyTIyT5IX3lr"},"outputs":[],"source":["num_classes = 10\n","toy_net = nn.Sequential(b1, b2, b3, b4, b5, nn.AdaptiveAvgPool2d((1, 1)),\n","                    nn.Flatten(), nn.Linear(512, 10))"]},{"cell_type":"markdown","metadata":{"id":"v8NoEuaAYDqe"},"source":["## ResNet-18\n","\n","In total, there are 4 convolutional layers in each module (excluding the 1×1 convolutional layer). Together with the first 7×7 convolutional layer and the final fully-connected layer, there are 18 layers in total. Therefore, this model is commonly known as ResNet-18.\n","The figure below displays this:\n","\n","![](https://drive.google.com/uc?export=view&id=1omcoC6FNmzWbRIi6W06dm6G9aPq3e_Ag)\n","\n","# ResNet-50, ResNet-101, ResNet152\n","The structure of ResNet is simple and quite easy to modify. Simply, by configuring different numbers of channels and residual blocks in the module, we can create different ResNet models, such as the deeper 152-layer ResNet-152. This is the reason that ResNet have been widely used by the community."]},{"cell_type":"markdown","metadata":{"id":"bF28Ho0pY-5O"},"source":["## Shape changes\n","\n","Before training, we observe how the input shape changes across different modules in ResNet. The resolution decreases while the number of channels increases up until the point where a global average pooling layer aggregates all features."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WdjfgTcwZIqm"},"outputs":[],"source":["X = torch.rand(size=(1, 1, 224, 224))\n","for layer in toy_net:\n","    X = layer(X)\n","    print(layer.__class__.__name__, 'output shape:\\t', X.shape)"]},{"cell_type":"markdown","metadata":{"id":"Z11ZdmbMZTCA"},"source":["## Training ResNet18\n","\n","First download CIFAR 10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lp7_-xYMaqsD"},"outputs":[],"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","torchvision.transforms.functional.resize\n","transform = transforms.Compose(\n","    [\n","     transforms.ToTensor(),\n","     transforms.Normalize((0.5,), (0.5,)),\n","])\n","\n","img_size = 28\n","batch_size = 64\n","\n","trainset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n","trainloader = torch.utils.data.DataLoader(trainset,batch_size=batch_size,shuffle=True,num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, transform=transform, download=True)\n","testloader = torch.utils.data.DataLoader(trainset,batch_size=batch_size,shuffle=False,num_workers=2)"]},{"cell_type":"markdown","metadata":{"id":"SEGqvKwuauox"},"source":["### Question 18\n","You need to create a ResNet class using the `BasicBlock` class from Question 5 and the methodology that we followed above."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DgWi30A4bVYA"},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch.nn import functional as F\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=1000, num_filters=64, input_dim=3):\n","        super(ResNet, self).__init__()\n","        self.inplanes = num_filters\n","        verbose = False\n","\n","        # first conv layer (b1)\n","        self.conv1 = nn.Conv2d(input_dim, num_filters, kernel_size=7, stride=2, padding=3, bias=False)\n","        self.bn1 = nn.BatchNorm2d(self.inplanes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","\n","        # all subsequent ones\n","\n","        # b2 = nn.Sequential(*_make_layer(64, 64, 2, first_block=True))\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1, first_block=True)\n","\n","        # b3 = ...\n","        num_filters *= 2\n","        self.layer2 = # TODO\n","\n","        # b4 = ...\n","        num_filters *= 2\n","        self.layer3 = # TODO\n","\n","        # b5 = ...\n","        num_filters *= 2\n","        self.layer4 = # TODO\n","\n","        # TODO\n","        # ...\n","\n","\n","    def _make_layer(self, block, input_channels, num_channels, num_residuals,\n","                 first_block=False):\n","        res_block = []\n","        for ii in range(num_residuals):\n","            # TODO\n","            # ...\n","            # remember: self.inplanes *=2 if ii = 0 and if not first block\n","\n","\n","        return # TODO ...\n","\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","\n","        x = self.layer1(x)\n","        # TODO\n","        # ...\n","\n","        return x\n"]},{"cell_type":"markdown","metadata":{"id":"RuDZ8Bvtc1ss"},"source":["### Question 19-a\n","\n","Train the above architecture using the code below:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G9E3bjXndXwH"},"outputs":[],"source":["MyNet = ResNet(BasicBlock, [2,2,2,2])\n","\n","use_cuda = True\n","if use_cuda and torch.cuda.is_available():\n","    print(\"using cuda\")\n","    MyNet.cuda()\n","learning_rate = 1e-3\n","optimizer = torch.optim.Adam(MyNet.parameters(),lr=learning_rate)\n","train(MyNet, optimizer, trainloader, testloader, criterion,  n_epoch = 5,\n","      train_acc_period = 10, test_acc_period = 1000)\n","\n","accuracy(MyNet, testloader, cuda=use_cuda)"]},{"cell_type":"markdown","metadata":{"id":"SK3NUM3vdual"},"source":["### Question 19-b\n","Compare the results with the pre-trained model from the original ResNet:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XBD9ETd7dxb-"},"outputs":[],"source":["import torchvision\n","from torchvision import *\n","\n","GT_resnet18 = models.resnet18(pretrained=True)\n","\n","use_cuda = True\n","if use_cuda and torch.cuda.is_available():\n","    print(\"using cuda\")\n","    GT_resnet18.cuda()\n","learning_rate = 1e-3\n","optimizer = torch.optim.Adam(GT_resnet18.parameters(),lr=learning_rate)\n","\n","train(GT_resnet18, optimizer, trainloader, testloader, criterion,  n_epoch = 5,\n","      train_acc_period = 100, test_acc_period = 1000)\n","accuracy(GT_resnet18, test_loader=testloader, cuda=use_cuda)"]},{"cell_type":"markdown","metadata":{"id":"ZzwAaSF4-7SL"},"source":["# InceptionNet (Bonus)\n","\n","The [InceptionNet](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/43022.pdf) architecture developed at Google won the ImageNet 2014 challenge with a top-5 error of 6.7%. A problem that appears in the design of a CNN architecture is the choice of the size of the convolutional kernels. More generally, in image processing, the question of which scale to detect patterns always arises. The InceptionNet architecture addresses this problem by relying on the notion of block inception. This is a multi-scale block allowing the network to choose between different scales / resolutions / pooling.\n","\n","The following code implements the InceptionNet architecure, as described in the GoogLeNet article (Figure 2b), and a truncated version of GoogLeNet:"]},{"cell_type":"markdown","metadata":{"id":"BvTueCmL_Uyc"},"source":["![](https://drive.google.com/uc?export=view&id=1RrZuOTGXU9VE9L9eRjiMCqwAW3gl57zB)"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"RiI8mdEc_iZm","executionInfo":{"status":"ok","timestamp":1736114452894,"user_tz":-60,"elapsed":11524,"user":{"displayName":"Lucas Degeorge","userId":"16095476420366092367"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","\n","class Inception(nn.Module):\n","    def __init__(self, in_planes, kernel_1_x, kernel_3_in, kernel_3_x, kernel_5_in, kernel_5_x, pool_planes):\n","        super(Inception, self).__init__()\n","        # 1x1 conv branch\n","        self.b1 = nn.Sequential(\n","            nn.Conv2d(in_planes, kernel_1_x, kernel_size=1),\n","            nn.BatchNorm2d(kernel_1_x),\n","            nn.ReLU(True),\n","        )\n","\n","        # 1x1 conv -> 3x3 conv branch\n","        self.b2 = nn.Sequential(\n","            nn.Conv2d(in_planes, kernel_3_in, kernel_size=1),\n","            nn.BatchNorm2d(kernel_3_in),\n","            nn.ReLU(True),\n","            nn.Conv2d(kernel_3_in, kernel_3_x, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(kernel_3_x),\n","            nn.ReLU(True),\n","        )\n","\n","        # 1x1 conv -> 5x5 conv branch\n","        self.b3 = nn.Sequential(\n","            nn.Conv2d(in_planes, kernel_5_in, kernel_size=1),\n","            nn.BatchNorm2d(kernel_5_in),\n","            nn.ReLU(True),\n","            nn.Conv2d(kernel_5_in, kernel_5_x, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(kernel_5_x),\n","            nn.ReLU(True),\n","            nn.Conv2d(kernel_5_x, kernel_5_x, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(kernel_5_x),\n","            nn.ReLU(True),\n","        )\n","\n","        # 3x3 pool -> 1x1 conv branch\n","        self.b4 = nn.Sequential(\n","            nn.MaxPool2d(3, stride=1, padding=1),\n","            nn.Conv2d(in_planes, pool_planes, kernel_size=1),\n","            nn.BatchNorm2d(pool_planes),\n","            nn.ReLU(True),\n","        )\n","\n","    def forward(self, x):\n","        y1 = self.b1(x)\n","        y2 = self.b2(x)\n","        y3 = self.b3(x)\n","        y4 = self.b4(x)\n","        return torch.cat([y1,y2,y3,y4], 1)\n","\n","class GoogLeNet(nn.Module):\n","    def __init__(self, input_dim=3):\n","        super(GoogLeNet, self).__init__()\n","        self.pre_layers = nn.Sequential(\n","            nn.Conv2d(input_dim, 192, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(192),\n","            nn.ReLU(True),\n","        )\n","\n","        self.layer1 = Inception(192,  64,  96, 128, 16, 32, 32)\n","\n","        self.layer2 = Inception(256, 128, 128, 192, 32, 96, 64)\n","\n","        self.layer3 = Inception(480, 192,  96, 208, 16,  48,  64)\n","\n","        self.max_pool = nn.MaxPool2d(3, stride=2, padding=1)\n","\n","        self.avgpool = nn.AvgPool2d(8, stride=1)\n","        self.linear = nn.Linear(512, 10)\n","\n","\n","    def forward(self, x):\n","        x = self.pre_layers(x)\n","\n","        x = self.layer1(x)\n","        x = self.max_pool(x)\n","        x = self.layer2(x)\n","        x = self.max_pool(x)\n","        x = self.layer3(x)\n","        x = self.avgpool(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.linear(x)\n","        return x\n"]},{"cell_type":"markdown","metadata":{"id":"KfUj6FWy_i8Q"},"source":["### Question 20 (Bonus)\n","\n","Test your GoogLeNet implementation on the FashionMNIST dataset:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MQkPJZCbkvy9"},"outputs":[],"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","transform = transforms.Compose(\n","    [transforms.Resize(size=(32, 32)),\n","     transforms.ToTensor(),\n","     transforms.Normalize((0.5,), (0.5,))])\n","\n","batch_size = 64\n","\n","trainset = torchvision.datasets.FashionMNIST(\"./data\",download=True,train=True,transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset,batch_size=batch_size,shuffle=True,num_workers=2)\n","\n","testset = torchvision.datasets.FashionMNIST(\"./data\",download=True,train=False,transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,shuffle=False, num_workers=2)\n","\n","def imshow(img):\n","    img = img / 2 + 0.5     # unnormalize\n","    npimg = img.numpy()\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","    plt.show()\n","\n","\n","# get some random training images\n","dataiter = iter(trainloader)\n","images, labels = next(dataiter)\n","\n","# show images\n","imshow(torchvision.utils.make_grid(images))"]},{"cell_type":"markdown","metadata":{"id":"B2nDtZYWVs8G"},"source":["## Colab Cheatsheet\n","\n","*   show keyboard shortcuts, Ctrl/Cmd M H, H\n","*   Insert code cell above, Ctrl/Cmd M A, A\n","*   Insert code cell below, Ctrl/Cmd M B, B\n","*   Delete cell/selection, Ctrl/Cmd M D, DD\n","*   Interrupt execution, Ctrl/Cmd M I, II\n","*   Convert to code cell, Ctrl/Cmd M Y, Y\n","*   Convert to text cell, Ctrl/Cmd M M, M\n","*   Split at cursor, Ctrl/Cmd M -, Ctrl Shift -\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":0}